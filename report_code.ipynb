{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "#### Preprocess Datasets to extract bounding boxes from segmentation masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess CODEBRIM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DATA_DIR = r'F:\\Accumulated_Defect_Segmentation\\CODEBRIM_original_images\\original_dataset\\annotations'\n",
    "files = os.scandir(DATA_DIR)\n",
    "for file in files:\n",
    "\n",
    "    if file.is_file():\n",
    "        with open(os.path.join(DATA_DIR, file.name), 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # Output File\n",
    "        outfile = open(os.path.join(DATA_DIR, \"YOLO_annotations\", file.name[:-4]+\".txt\"), \"w\")\n",
    "        crack, spalling, efflorescence, corrosion = False, False, False, False\n",
    "        h, w = 0, 0\n",
    "\n",
    "        print(\"Parsing: \" + file.name)\n",
    "        Bs_data = BeautifulSoup(data, \"xml\")\n",
    "\n",
    "        img_res = Bs_data.find('size')\n",
    "        h, w = int(img_res.height.string), int(img_res.width.string)\n",
    "\n",
    "        for obj in Bs_data.find_all(\"object\"):\n",
    "            crack, spalling, efflorescence, corrosion = False, False, False, False\n",
    "\n",
    "            xmin, ymin, xmax, ymax = int(obj.bndbox.xmin.string), int(obj.bndbox.ymin.string), int(obj.bndbox.xmax.string), int(obj.bndbox.ymax.string)\n",
    "            width, height = int(xmax - xmin), int(ymax - ymin)\n",
    "            x_centre, y_centre = xmin + width/2, ymin + height/2\n",
    "\n",
    "            # Convert to Percentile\n",
    "            x_centre, y_centre, width, height = x_centre/w, y_centre/h, width/w, height/h\n",
    "\n",
    "            crack, spalling, efflorescence = bool(int(obj.Defect.Crack.string)), bool(int(obj.Defect.Spallation.string)), bool(int(obj.Defect.Efflorescence.string))\n",
    "\n",
    "            if bool(obj.Defect.ExposedBars.int) or bool(int(obj.Defect.CorrosionStain.string)):\n",
    "                corrosion = True\n",
    "\n",
    "            # Write in outfile\n",
    "            if crack:\n",
    "                outfile.write(\"0 \" + str(x_centre) + \" \" + str(y_centre) + \" \" + str(width) + \" \" + str(height)+\"\\n\")\n",
    "            elif spalling:\n",
    "                outfile.write(\"1 \" + str(x_centre) + \" \" + str(y_centre) + \" \" + str(width) + \" \" + str(height)+\"\\n\")\n",
    "            elif efflorescence:\n",
    "                outfile.write(\"3 \" + str(x_centre) + \" \" + str(y_centre) + \" \" + str(width) + \" \" + str(height)+\"\\n\")\n",
    "            else:\n",
    "                if corrosion:\n",
    "                    outfile.write(\"2 \" + str(x_centre) + \" \" + str(y_centre) + \" \" + str(width) + \" \" + str(height)+\"\\n\")\n",
    "\n",
    "        outfile.close()\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess QuakeCity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.measure import label, regionprops, find_contours\n",
    "from skimage.morphology import dilation, closing\n",
    "\n",
    "\"\"\" Creating a directory \"\"\"\n",
    "\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "\"\"\" Convert a mask to border image \"\"\"\n",
    "\n",
    "\n",
    "def mask_to_border(mask):\n",
    "    h, w = mask.shape\n",
    "    border = np.zeros((h, w))\n",
    "\n",
    "    contours = find_contours(mask)\n",
    "    for contour in contours:\n",
    "        for c in contour:\n",
    "            x = int(c[0])\n",
    "            y = int(c[1])\n",
    "            border[x][y] = 255\n",
    "\n",
    "    return border\n",
    "\n",
    "\n",
    "\"\"\" Mask to bounding boxes \"\"\"\n",
    "\n",
    "\n",
    "def mask_to_bbox(mask):\n",
    "    bboxes = []\n",
    "\n",
    "    mask = mask_to_border(mask)\n",
    "    lbl = label(mask)\n",
    "    props = regionprops(lbl)\n",
    "    for prop in props:\n",
    "        x1 = prop.bbox[1]\n",
    "        y1 = prop.bbox[0]\n",
    "\n",
    "        x2 = prop.bbox[3]\n",
    "        y2 = prop.bbox[2]\n",
    "\n",
    "        bboxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def parse_mask(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)\n",
    "    return mask\n",
    "\n",
    "def plot_bbox(img, bboxes, c):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    for bbox in bboxes:\n",
    "        x = bbox[0] * img_w\n",
    "        y = bbox[1] * img_h\n",
    "        w = bbox[2] * img_w\n",
    "        h = bbox[3] * img_h\n",
    "\n",
    "        x1 = int(x - w / 2)\n",
    "        y1 = int(y - h / 2)\n",
    "        x2 = int(x + w / 2)\n",
    "        y2 = int(y + h / 2)\n",
    "        \n",
    "\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), c, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def remove_small_boxes(bboxes, min_size = 50):\n",
    "    temp = bboxes\n",
    "    for bbox in bboxes:\n",
    "        width = bbox[2] - bbox[0]\n",
    "        height = bbox[3] - bbox[1]\n",
    "        if width < min_size or height < min_size:\n",
    "            temp.remove(bbox)\n",
    "    return temp\n",
    "\n",
    "def write_outfile(bboxes, classification, outfile, w, h):\n",
    "    for bbox in bboxes:\n",
    "        xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        width, height = int(xmax - xmin), int(ymax - ymin)\n",
    "        x_centre, y_centre = xmin + (width/2), ymin + (height/2)\n",
    "\n",
    "        # Convert to Percentile\n",
    "        x_centre, y_centre, width, height = x_centre / w, y_centre / h, width / w, height / h\n",
    "\n",
    "        outfile.write(str(classification) + \" \" + str(x_centre) + \" \" + str(y_centre) + \" \" + str(width) + \" \" + str(height) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Load the dataset \"\"\"\n",
    "    DATA_DIR = r'F:\\Accumulated_Defect_Segmentation\\UH_QuakeCity\\QuakeCity\\label'\n",
    "    IMG_DIR = r'F:\\Accumulated_Defect_Segmentation\\UH_QuakeCity\\QuakeCity\\image'\n",
    "\n",
    "    \"\"\" Create dir \"\"\"\n",
    "    create_dir(os.path.join(DATA_DIR, \"YOLO_annotations\"))\n",
    "\n",
    "    files = os.scandir(os.path.join(DATA_DIR, \"crack\"))\n",
    "    for file in files:\n",
    "\n",
    "        print(\"Processing \"+file.name)\n",
    "\n",
    "        name = file.name.split(\".\")[0]\n",
    "\n",
    "        # Output File\n",
    "        outfile = open(os.path.join(DATA_DIR, \"YOLO_annotations\", name+\".txt\"), \"w\")\n",
    "\n",
    "        \"\"\" Read image and mask \"\"\"\n",
    "        image = cv2.imread(os.path.join(IMG_DIR, file.name), cv2.IMREAD_COLOR)\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        crack_mask = cv2.imread(os.path.join(DATA_DIR, \"crack\", file.name), cv2.IMREAD_GRAYSCALE)\n",
    "        rebar_mask = cv2.imread(os.path.join(DATA_DIR, \"rebar\", file.name), cv2.IMREAD_GRAYSCALE)\n",
    "        spall_mask = cv2.imread(os.path.join(DATA_DIR, \"spall\", file.name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        \"\"\" Detecting Bounding Boxes \"\"\"\n",
    "        crack_mask = closing(dilation(dilation(dilation(dilation(dilation(closing(closing(crack_mask))))))))\n",
    "        crack_bboxes = mask_to_bbox(crack_mask)\n",
    "        rebar_mask = closing(dilation(dilation(dilation(dilation(dilation(closing(closing(rebar_mask))))))))\n",
    "        rebar_bboxes = mask_to_bbox(rebar_mask)\n",
    "        spall_mask = closing(dilation(dilation(dilation(dilation(dilation(closing(closing(spall_mask))))))))\n",
    "        spall_bboxes = mask_to_bbox(spall_mask)\n",
    "\n",
    "        \"\"\" Remove Too Small Boxes \"\"\"\n",
    "        crack_bboxes = remove_small_boxes(crack_bboxes)\n",
    "        rebar_bboxes = remove_small_boxes(rebar_bboxes)\n",
    "        spall_bboxes = remove_small_boxes(spall_bboxes)\n",
    "\n",
    "        \"\"\" Write to File \"\"\"\n",
    "        write_outfile(crack_bboxes, 0, outfile, w, h)\n",
    "        write_outfile(rebar_bboxes, 2, outfile, w, h)\n",
    "        write_outfile(spall_bboxes, 1, outfile, w, h)\n",
    "\n",
    "        outfile.close()\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess S2DS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "DATASET_DIR = '/home/zaid/datasets/defect_segment/s2ds'\n",
    "OUTPUT_DIR = '/home/zaid/datasets/defect_segment/s2ds_out'\n",
    "\n",
    "# white: cracks\n",
    "# red: spalling\n",
    "# yellow: corrosion\n",
    "# blue: efflorescence\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'check'), exist_ok=True)\n",
    "\n",
    "    train = sorted(glob.glob(os.path.join(dataset_dir, \"train\", \"*\")))\n",
    "    test = sorted(glob.glob(os.path.join(dataset_dir, \"test\", \"*\")))\n",
    "    val = sorted(glob.glob(os.path.join(dataset_dir, \"val\", \"*\")))\n",
    "    images = train + test + val\n",
    "    n = len(images)\n",
    "\n",
    "    for i,img in enumerate(images):\n",
    "        if '_lab' in img:\n",
    "            continue        \n",
    "\n",
    "        print(f\"Processing image {i} of {n}\", end='\\r')\n",
    "\n",
    "        img_name = img.split('/')[-1]\n",
    "        shutil.copy(img, os.path.join(output_dir, 'images', img_name))\n",
    "        \n",
    "        mask = cv2.imread(img.replace('.png', '_lab.png'), cv2.IMREAD_COLOR)\n",
    "        mask_crack = cv2.inRange(mask, np.array([255, 255, 255]), np.array([255, 255, 255]))\n",
    "        mask_spall = cv2.inRange(mask, np.array([0, 0, 255]), np.array([0, 0, 255]))\n",
    "        mask_eff = cv2.inRange(mask, np.array([255, 255, 0]), np.array([255, 255, 0]))\n",
    "        mask_corr = cv2.inRange(mask, np.array([0, 255, 255]), np.array([0, 255, 255]))        \n",
    "\n",
    "        label_cracks = mask_to_bbox(mask_crack)\n",
    "        label_spall = mask_to_bbox(mask_spall)\n",
    "        label_eff = mask_to_bbox(mask_eff)\n",
    "        label_corr = mask_to_bbox(mask_corr)\n",
    "\n",
    "        with open(os.path.join(output_dir, 'labels', img_name.replace('.png', '.txt')), 'w') as f:\n",
    "            for label in label_cracks:\n",
    "                f.write('0 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "            for label in label_spall:\n",
    "                f.write('1 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "            for label in label_corr:\n",
    "                f.write('2 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "            for label in label_eff:\n",
    "                f.write('3 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "\n",
    "        img_check = plot_bbox(cv2.imread(img, cv2.IMREAD_COLOR), label_cracks, (255, 255, 255))\n",
    "        img_check = plot_bbox(img_check, label_spall, (0, 0, 255))\n",
    "        img_check = plot_bbox(img_check, label_corr, (0, 255, 255))\n",
    "        img_check = plot_bbox(img_check, label_eff, (255, 255, 0))\n",
    "\n",
    "        cv2.imwrite(os.path.join(output_dir, 'check', img_name.replace('.png', '_check.png')), img_check)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(DATASET_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Zhang dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "DATASET_DIR = '/home/zaid/datasets/defect_segment/zhang_defect_segmentation'\n",
    "OUTPUT_DIR = '/home/zaid/datasets/defect_segment/zhang_defect_segmentation_out'\n",
    "\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'check'), exist_ok=True)\n",
    "\n",
    "    images = sorted(glob.glob(os.path.join(dataset_dir, \"images\", \"*\")))\n",
    "    n = len(images)\n",
    "\n",
    "    for i,img in enumerate(images):\n",
    "        if '_lab' in img:\n",
    "            continue        \n",
    "\n",
    "        print(f\"Processing image {i} of {n}\", end='\\r')\n",
    "\n",
    "        img_name = img.split('/')[-1]\n",
    "        shutil.copy(img, os.path.join(output_dir, 'images', img_name))\n",
    "\n",
    "        img_id  = img_name.split('.')[0]                \n",
    "                \n",
    "        f_crack = os.path.join(dataset_dir, 'masks', img_id+'crack'+'.jpg')\n",
    "        f_spall = os.path.join(dataset_dir, 'masks', img_id+'spall'+'.jpg')\n",
    "        f_corr = os.path.join(dataset_dir, 'masks', img_id+'rebar'+'.jpg')\n",
    "        \n",
    "        if not os.path.exists(f_crack):\n",
    "            label_cracks = []\n",
    "        else:\n",
    "            mask_crack = cv2.imread(f_crack, cv2.IMREAD_GRAYSCALE)\n",
    "            label_cracks = mask_to_bbox(mask_crack)\n",
    "\n",
    "        if not os.path.exists(f_spall):\n",
    "            label_spall = []\n",
    "        else:\n",
    "            mask_spall = cv2.imread(f_spall, cv2.IMREAD_GRAYSCALE)\n",
    "            label_spall = mask_to_bbox(mask_spall)\n",
    "        \n",
    "        if not os.path.exists(f_corr):\n",
    "            label_corr = []\n",
    "        else:\n",
    "            mask_corr  = cv2.imread(f_corr, cv2.IMREAD_GRAYSCALE)\n",
    "            label_corr = mask_to_bbox(mask_corr)\n",
    "\n",
    "        with open(os.path.join(output_dir, 'labels', img_id + '.txt'), 'w') as f:\n",
    "            for label in label_cracks:\n",
    "                f.write('0 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "            for label in label_spall:\n",
    "                f.write('1 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "            for label in label_corr:\n",
    "                f.write('2 ' + ' '.join([str(x) for x in label]) + '\\n')\n",
    "\n",
    "\n",
    "        img_check = plot_bbox(cv2.imread(img, cv2.IMREAD_COLOR), label_cracks, (255, 255, 255))\n",
    "        img_check = plot_bbox(img_check, label_spall, (0, 0, 255))\n",
    "        img_check = plot_bbox(img_check, label_corr, (0, 255, 255))\n",
    "\n",
    "        cv2.imwrite(os.path.join(output_dir, 'check', img_name.replace('.png', '_check.png')), img_check)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(DATASET_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models Training\n",
    "#### Train YOLO V8 Models using RoboFlow and ultralytics on each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODEBRIM_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "ultralytics.checks()\n",
    "\n",
    "rf = Roboflow(api_key=\"YOUR API KEY\")\n",
    "os.chdir('./datasets')\n",
    "project = rf.workspace(\"cvisslab\").project(\"codebrim-poidd\")\n",
    "dataset = project.version(2155).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuakeCity_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "ultralytics.checks()\n",
    "\n",
    "rf = Roboflow(api_key=\"YOUR API KEY\")\n",
    "os.chdir('./datasets')\n",
    "project = rf.workspace(\"cvisslab\").project(\"quakecity\")\n",
    "dataset = project.version(1).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S2DS_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "ultralytics.checks()\n",
    "\n",
    "rf = Roboflow(api_key=\"YOUR API KEY\")\n",
    "os.chdir('./datasets')\n",
    "project = rf.workspace(\"cvisslab\").project(\"s2ds\")\n",
    "dataset = project.version(1).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhang.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "ultralytics.checks()\n",
    "\n",
    "rf = Roboflow(api_key=\"YOUR API KEY\")\n",
    "os.chdir('./datasets')\n",
    "project = rf.workspace(\"cvisslab\").project(\"zhang-3seb8\")\n",
    "dataset = project.version(1).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DockerFile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FROM nvidia/cuda:11.6.2-devel-ubuntu20.04\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "ENV TZ=America/New_York\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y tar git curl nano wget dialog net-tools build-essential\n",
    "RUN apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\n",
    "\n",
    "\n",
    "RUN apt-get install -y python3.8\n",
    "RUN ln -s /usr/bin/python3.8 /usr/bin/python\n",
    "RUN apt-get install -y python3-pip\n",
    "#RUN apt-get install -y python-dev python-setuptools\n",
    "\n",
    "RUN python --version \n",
    "\n",
    "RUN mkdir projects\n",
    "WORKDIR /projects/\n",
    "\n",
    "RUN mkdir /projects/datasets\n",
    "\n",
    "RUN pip install ultralytics==8.0.20\n",
    "RUN pip install roboflow\n",
    "\n",
    "RUN apt install -y vim\n",
    "\n",
    "COPY ./*.py /projects/\n",
    "ADD ./results /projects/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Docker Container"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker run --shm-size 15gb -it -v ./results:/projects/results msci623yolov8:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yolo task=detect mode=train model=yolov8s.pt data=./datasets/CODEBRIM-2155/data.yaml epochs=50 imgsz=1024 batch=-1 patience=10 device=0 plots=True\n",
    "yolo task=detect mode=train model=yolov8s.pt data=./datasets/S2DS-2155/data.yaml epochs=50 imgsz=1024 batch=-1 patience=10 device=0 plots=True\n",
    "yolo task=detect mode=train model=yolov8s.pt data=./datasets/QuakeCity-2155/data.yaml epochs=50 imgsz=1024 batch=-1 patience=10 device=0 plots=True\n",
    "yolo task=detect mode=train model=yolov8s.pt data=./datasets/Zhang-2155/data.yaml epochs=50 imgsz=1024 batch=-1 patience=10 device=0 plots=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Validation\n",
    "\n",
    "#### Validate baseline models and run cross-validation between each model and each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuakeCity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yolo task=detect mode=val model=./results/QuakeCity_baseline/detect/trainn/weights/best.pt data=./datasets/Zhang-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/QuakeCity_baseline/detect/trainn/weights/best.pt data=./datasets/S2DS-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/QuakeCity_baseline/detect/trainn/weights/best.pt data=./datasets/CODEBRIM-1/data.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhang"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yolo task=detect mode=val model=./results/Zhang_baseline/detect/trainn/weights/best.pt data=./datasets/QuakeCity-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/Zhang_baseline/detect/trainn/weights/best.pt data=./datasets/S2DS-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/Zhang_baseline/detect/trainn/weights/best.pt data=./datasets/CODEBRIM-1/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODEBRIM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yolo task=detect mode=val model=./results/CODEBRIM_baseline/detect/trainn/weights/best.pt data=./datasets/QuakeCity-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/CODEBRIM_baseline/detect/trainn/weights/best.pt data=./datasets/S2DS-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/CODEBRIM_baseline/detect/trainn/weights/best.pt data=./datasets/Zhang-1/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S2DS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yolo task=detect mode=val model=./results/S2DS_baseline/detect/trainn/weights/best.pt data=./datasets/QuakeCity-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/S2DS_baseline/detect/trainn/weights/best.pt data=./datasets/CODEBRIM-1/data.yaml\n",
    "yolo task=detect mode=val model=./results/S2DS_baseline/detect/trainn/weights/best.pt data=./datasets/Zhang-1/data.yaml"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
